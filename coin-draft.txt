## Neural ensemble communities: Open-source approaches to hardware for large scale recording

Siegle JH, Hale G, Newman JP, Voigts J


Scientists preparing to test a hypothesis that would benefit from novel methods are commonly faced with a choice: Either invest in the training or personnel needed to build new tools for their lab, or outsource these efforts to companies that can sell them finished devices for obtaining quick and reliable results. Commercial equipment is often expensive, but it can reduce time to publication by eliminating the need to design and test customized lab kit.

Different fields gravitate to different ends of this spectrum between building and buying their tools. In cases in which the available technology changes slowly, commercial tools can improve efficiency by allowing researchers to focus on science rather than tool making. But there are instances in which commercial tools can hinder progress. Commercial tools are usually sold without access to the schematics or source code required to check the validity of results or to re-run experiments and analyses with different parameters. For this reason, they are often treated as "black box" commodities that can be purchased but not modified, which limits the understanding of resulting data and extensibility of each tools' capabilities. As a result, experiments are often adjusted to fit the available methods rather than vice versaâ€”especially in cases where experimental needs weren't anticipated at the time when equipment was purchased. Furthermore, tools from different companies, even those designed for the same function, are often incompatible with one another. Once a system is chosen, future work may end up "locked in" to a particular system.

In fields where technical development is closely linked with the progress of scientific ideas, researchers are forced to build or modify their own tools in order to keep pace with their competitors. In such an environment, tools are often developed on the fly with no standardization or quality control, serving only the purpose of the current experiment. This approach sacrifices quality and reliability, but generally works because experimenters are intimately familiar with the function of their particular experiment. A similar situation often arises in labs that lack the funding for commercial tools, or in labs that want to add new techniques to their repertoire without committing a lot of resources.

In this article, we examine the role that open-source development can play in extracellular electrophysiology, a widely used technique in neuroscience. We believe that, when designed properly, open-source tools can provide the best of both worlds by combining the quality, ease of use, and support of commercial products with the high performance and adaptability of locally developed tools.

Extracellular recording electrodes were one of the earliest tools for investigating brain function, and are now one of the most rapidly evolving. We believe that the community of scientists using these methods is ideally positioned to benefit from an open approach to technical development. A recent wave of open-source tools for electrophysiology provide a useful starting point{CITATIONS}. Crucially, the parallel growth of platforms for collaborative design and affordable standardized manufacturing have now recently made the open-source model competitive with commerical options. Below, we present arguments for and against the need for open-source hardware for high-channel-count electrophysiology, which also apply to other neuroscientific techniques.

#2A Electrophysiology is well-suited for an open development model

Recording electrical signals from the brain is a straightforward proposition. In the simplest case, it only requires two conductors to measure a potential difference, a means of amplifying that difference, and a method to store changes in the amplified signal over time. A century ago, nerve impulses were being amplified using vacuum tubes and recorded on photographic film scanned behind a mercury column {Adrian 1926}. Today, mass-produced circuits costing a few dollars can be used to amplify neural signals and store them digitally{CITATION}.

While the technology for recording of neural signals has remained almost unchanged [this is not true as stated], the electrodes used to detect these signals, as well as the computational algorithms used to manipulate them, are evolving rapidly. In recent years, there has been a push to record from more channels simultaneously in order to understand the brain at the network level{CITATION - brain project etc}. Further, the emergence of increasingly specific ways of manipulating neural activity{CITATION} and a push towards more complex behavioural experiments requires equipment in which experiments are changed based on behavioural and neural data in real-time instead of being pre-determined.

For the most part, advances in recording technology occur within individual labs. Companies then take these advances, optimize them for general usability, and distribute them to the wider community. The resulting systems are typically monolithic, in that all the parts are designed by the same company, and closed, in that the hardware designs and source code are not publicly available. Some of the major vendors of commercial data acquisition systems are Neuralynx, Plexon, Blackrock, Tucker-Davis Technologies, Ripple, and Axona [any others to add???]. By giving researchers access to high-quality, professionally tested tools, as well as reliable support services, these companies have been essential to the spread of multichannel electrophysiology systems over the past two decades. However, it is no longer clear that these services should be provided exclusively by commercial entities.

We see three reasons why the current model of tool development and distribution would benefit from an active open-source community:

1. Electrode technology is advancing rapidly. Experimenters using twisted-wire tetrodes are packing more wires into a smaller area{CITATIONS}; silicon probes are becoming thinner and denser{CITATIONS}; and active probes are under development{CITATIONS}. Researchers need the flexibility to choose the option that is best-suited to their particular application, and need to be able to mix-and match electrode types, but vendor lock-in can prevent this. Companies have trouble keeping up with the latest advances, and when they do, they often implement proprietary standards that Balkanize the field. One recent example of this is the development of amplifier chips from Intan technologies, which can amplify and digitize 32 channels of neural data in an 8 x 8 mm package. When integrated into a "headstage" (the temporary interface that connects implanted electrodes to a data acquisition system), systems based on Intan chips offer considerable advantages over their analog counterparts. For this reason, nearly every major vendor now sells headstages that use Intan chips for digitization, but none of them are interchangeable. Users are stuck with whatever connectors the vendors have chosen to provide, and cannot customize them without the help of the manufacturer.

2. On the software side, the requirements for analysis and visualization vary greatly from researcher to researcher, and even from experiment to experiment. Specialized algorithms are needed to handle electrophysiological data once it reaches the computer, especially when closed-loop feedback is involved. It's often difficult for researchers to predict which algorithms will be necessary before the experiments have been run. An example of this is online spike-sorting, which allows researchers to probe the response properties of single units near their recording electrode. Certain commercial software may or may not implement this, and they may all use different algorithms. Further, the exact mathematical method used to project spike waveforms into a feature space and segmenting feature clusters is often not disclosed even though this process varies widely from vendor to vendor, can can affect the interpretation of recordings~\citep{Cohen2011}. This makes it difficult or impossible to directly compare data collected across different data acquisition platforms platforms.

3. There is currently large amounts of redundant tool development going on within each lab. Electrophysiologists tend to be technically savvy and favor a "do it yourself" approach to their work. Some of this is cultural, but much of it is out of necessity. The complexity of neurological systems combined with the limiting nature of commercially available products has forced many electrophysiologists to take matters into their own hands, and develop customized hardware and software for their experiments. Unfortunately, very little of this development is shared, leading to a huge amount of redundant effort within and across laboratories.

These reasons, which are not unique to extracellular electrophysiology, make it likely that a shift toward a more open development model will occur in the near future.

#2B A brief history of open-source approaches

The development of custom tools has always been a fact of life in extracellular electrophysiology. Sometimes it's done out of necessity, when commercial suppliers simply do not exist; other times it's done out of principle, to provide peace of mind to researchers who want to know how their tools work at every level. Since the widespread adoption of multichannel recording techniques, there have been several attempts to develop open-source recording platforms that are polished enough and sufficiently well documented to propagate beyond the labs that invented them.


## A/D
In the early 1990s, A/D... (someone should ask Matt about this to get the history right).

Should we mention the large time gap? Or are there tools we don't know about?
JPN - I'm going to interview Daniel Wagenaar and Bruce Wheeler, two of the early technically minded people in the planar MEA world, to see if they have any ideas about this. Are their others to talk to about the in vivo side of things. Its important to remember that during this time github etc did not exist, so code sharing was pretty ad hoc. I think mentioning this would be good because it indicates that the growth of infrastructure supporting open hardware and software mirrored the popularity of the idea in general.

## MEABench
MEABench is a set of Linux command line programs for acquiring, processing, and saving multielectrode voltages from planar microelectrode arrays (MEAs). MEABench was created in 2005 by Daniel Wagenaar, Tom Demarse, and Steve Potter. Each MEABench program applies a single function, such as 'Filter' or 'Record', to a multichannel data stream. Each program uses standardized inter-process streaming interface, allowing programs to be daisy-chained and branched in order to construct complex signal processing arrangements. Although MEABench does not provide native support for closed-loop experiments, it can combined with real-time simulation tools~\citep{Wagenaar2002} using custom â€˜glueâ€™ programs to create feedback loops~\citep{Wagenaar2005}. MEABench has limited hardware driver support and currently only works with outdated and very expensive MCS data acquisition cards. However, the modularity and MEABench and reconfigurability of MEABench have inspired more modern, extensible, easy-to-use open-source solutions.

## NeuroRighter
The introduction high channel count data aquisition cards produced by National Instuments in the mid-2000's led the Potter lab to develop a second open-source electrophysiology platform called NeuroRighter. NeuroRighter was created by John Rolson, Jon Newman, and Riley Zeller-Townson. NeuroRighter significantly reduced the cost of multichannel data acqusition for MEAs compared to MEABench from ~65,000 to ~10,000 USD. To increase usability compared to MEABench, NeuroRighter operates as a standalone application with graphical control over filter and amplifier settings, online spike-sorting, data visualization, and data storage {Rolston 2009}. Further, NeuroRighter integrated native support for real-time feedback. NeuroRighter's data processing pipeline can be augmented using an appilcation programing interface to create 'plugin' libraries that can be executed by NeuroRighter as it operates {Newman 2012}. The NeuroRigher API also supports electrical and optical stimulation protocols, so that closed-loop experimentation is possible.

## ArtE
2010 - ArtE - Greg should write this

The public release of integrated amplifier chips by Intan Technologies in 2009?{CITATION} made it possible to circumvent the National Instruments analog-to-digital conversion hardware that was a part of all the previous platforms. The co-founders of the Open Ephys initiative, Josh Siegle and Jakob Voigts, designed a system based on these chips, with reduced hardware complexity and an order of magnitude drop in equipment cost compared to ArtE and NeuroRighter. The development and promotion of open interfaces by Intan (RHD2000 SPI protocol and Rhythm FPGA firmware) made the process much simpler by reducing the number of design decisions and making their tools compatible with pre-existing hardware and software from Intan. The low price to manufacture each acquisition board (~$700 per unit in bulk), allowed Open Ephys to distribute the tools widely in a short period of time. There are currently over 80 labs with Open Ephys hardware.

What drove the development and increased adoption of these open source tools? There are a few key factors that have recently allowed open-source tools to rival (and in some ways, surpass) the functionality of their commercial counterparts. First of, thanks to their openness, all of the listed systems were to some degree built upon and improving upon each-other's designs. NeuroRighter was created to simplify MEABench, ArtE was inspired by the efforts of NeuroRighter, and the Open Ephys software began as a graphical interface for ArtE. Different requirements caused these systems to diverge, but there is no reason that they couldn't be made cross-compatible, or continue to benefit from cross-pollination of ideas.

[This paragraph needs to be made more coherent and logical:]
In parallel to this, developments not directly related to neuroscience contributed to the increasing quality of open-source tools.

1. Smaller, cheaper hardware. Market forces are pushing for ever smaller and cheaper processors and other components that are contained in cellular phones and portable devices. When these become publicly available, they simplify the design process for neuroscientists.

2. Tools for collaborative design. The rise of the Internet, which led to an explosion of tools for "social design." GitHub is a website that makes it easy for researchers to collaboratively develop software and hardware, based on the powerful git software written by Linus Torvalds, the creator of Linux. Wikis allow documentation to be distributed across the community, and to be updated instantly. On a more mundane level, email and video conferencing make it more common to trust developers that one hasn't ever met in person. 

3. The open-source hardware movement. Products like Arduino, Raspberry Pi, and Beaglebone make it easy to harness high-powered embedded computations at a low price, and low barrier to entry. Many neuroscientists get their start in hardware design using simple platforms like Arduino, and then graduate to more powerful systems. They also set a precedent for what good open-source design should be: simple to comprehend, highly adaptable, and well-documented.

#2C Open interfaces: a middle-of-the-road solution

Taking cues from these widely adopted open-source platforms, we propose an open approach to hardware and software development for extracellular electrophysiology that centers around standardized interfaces and modular architecture. If it's embraced by both researchers and commercial suppliers, it could greatly improve the productivity of the scientific community. The essence of this proposal is that the most common interfaces (e.g. electrode-to-headstage, headstage-to-cable, data-to-computer) become standardized, so that anyone can make tools that fit into the pipeline. This is what already exists in many other ecosystems, such as audio recording, where microphones, analog-to-digital converters, and processing software can all come from different companies, yet all work together seamlessly.

[CREATE A FIGURE FOR THIS â€“Â JAKOB???]

There is no reason to circumvent the technical expertise accumulated by commercial tool developers. In a model where system components are modular, well documented, and interchangeable, companies could diversify and concentrate their resources. Rather than developing and supporting entire systems, they could focus on making the highest-quality components within a modular system. This development could be done in collaboration with scientists that come up with ideas for require new tools, or already have prototypes that aren't ready for distribution. Additionally, standardized equipment should give rise to an even bigger market for professional support for existing systems, similar to how companies sell support for Linux based systems rather than commoditizing the software itself.

There is also no fundamental reason why all components of electrophysiology systems need to be open source either. Even the most "open" tools make heavy use of "black boxes" in the form of well-documented, but closed-source integrated circuits. These circuits perform specified functions ranging from amplifiers and filters up to FPGAs and processors. As long as the behavior of a component is well defined, and its interfaces are documented and adhere to standards wherever possible, closed-source components are not necessarily an obstacle to the functionality of the whole system. The key requirement is that tools need to allow researchers to choose their path, and have options at each point.

The same principle applies to the software used to visualize and record data streaming from the hardware. Currently, the software provided by commercial companies is tied to specific hardware. There is no general-purpose software for neuroscience experiments, which leads to a huge amount of redundancy and lock-in. For the same reasons that we need modular hardware, modular software will become crucial in coming years. Higher channel counts demand that processing be done in real time, and many experiments would benefit from closed-loop processing. If there's no standard interface for this, researchers will end up replicating the same code many times for many different platforms. And the more processing that is done in real time, the more difficult it will be to compare the results of experiments collected on different platform.


#2D Pros and cons of the open-source model

Actually, none of this is very clear-cut. We can't really divide it up into pros and cons

PRICE
- Open-source is cheaper in principle, since you can build it from raw materials
- But development costs may be high (especially when you factor in time spent); no economies of scale if you're building a one-off system (examples?)
- Sometimes open-source systems are the only choice, especially in labs with low budgets, or those that do not specialize in ephys

SUPPORT
- Commercial systems offer the advantage of guaranteed support, but how well does this work in practice?
- With open-source, support can be distributed throughout the community, but obviously this depends on the size and engagement of the community

DOCUMENTATION
- Wiki-style approach is very powerful (example: Open Ephys and NeuroRighter)
- But unless someone is in charge, this can be messy and disorganized
- HOWEVER, are there any examples of commercial systems with great docs?

PERFORMANCE
- Again, commercial systems are generally perceived to be higher-quality
- However, often the components are identical, so it's not clear where this quality comes from (example of Intan chips)
- We recommend benchmarking everything yourself

FLEXIBILITY
- Open-source systems clearly win in this case, but often require lots of expertise to actually contribute
- Best-case scenario would be the open interfaces described above â€“Â researchers could mix and match different elements of the pipeline, and make adjustments to only the parts that are necessary to change for their experiment

#3 Conclusion

[Taken from outline]:
Open should be the rule for large-scale recording, rather than the exception
If we can convince scientists and companies to support open standards, we'll spend less time on redundant hardware and software development
There is a place for closed-source hardware, but scientists should demand openness wherever possible, especially when it comes to areas that are rapidly transforming
Closed-source is OK, however, as long as you know how to interface with it (and the interfaces are at the right level of abstraction)
We are not opposed to closed-source tools, only ones that hinder progress (what are our criteria for this, exactly?)

Key takeaway: open tools will ultimately result in less time spent on development, but only if they are actively promoted and financially supported by the labs/funding agencies that benefit from them




