## Neural ensemble communities: Open-source approaches to hardware for large scale recording

Siegle JH, Hale G, Newman JP, Voigts J



Scientists preparing to test a hypothesis that requires novel methods are commonly faced with a choice: Either spend considerable time and money to gain the expertise and possibly hire personnel to build and run the necessary experiments, or outsource some of these efforts to companies that provide finished methods that lead to quick and reliable results. Commerical equipment is usually expensive, but generally provides high quality results and reduce time to publication.

Different fields gravitate to different ends of this spectrum between building and buying their tools. In the majority of cases where methods doesn't change much, commercial tools provide efficiency by allowing researchers to focus on the science. But there are certain instances in which commerical tools hinder progress. Commercial scientific tools are usually sold without access to all the raw data or code required to check the validity of results or to re-run experiments and analyses with different parameters. Scientists are often happy to treat them as "black box" commodities that can be purchased but not modified, which limits the understanding of the resulting data. As a result, experiments are often adjusted to fit the available methods rather than the other way round- especially in cases where experimental needs weren't anticipated at the time when equipment was purchased. Furthermore, tools from different companies may be incompatible with one another. Once a system is chosen, future work may end up "locked in" to one system.

On the other end of the spectrum are fields where technical development is closely linked with the progress of scientific ideas. When methods evolve quickly, researchers are forced to build or modify their own tools in order to keep up with current ideas. In such an environment, tools are often developed on the fly with no standardization or quality control, serving only the purpose of the current experiment. This approach sacrifices quality and reliability but generally works out because experimenters are intimately familiar with the function of their perticular experiment. A similar situation often arises in labs that lack the funding for commercial tools, or in labs that want to add new techniques to their repertoire without commiting a lot of ressources.

In this article, we examine the role that open-source development can play in extracellular electrophysiology, a widely used technique in neuroscience. We believe that open source tools can provide the best of both worlds, combining the quality, ease of use and support of commercial products with the high performance and adaptability of locally developed tools.

Extracellular recording electrodes were one of the earliest tools for investigating brain function, and are now one of the most rapidly evolving. We believe that the community of scientists using these methods is ideally positioned to benefit from an open approach to technical development. The recent wave of open-source tools for electrophysiology provide a useful starting point{CITATIONS}. Most importantly, novel platforms for collaborative design and affordable standardized manufacturing make such a model feasible for the first time. Below, we present arguments for and against the need for open-source hardware for high-channel-count electrophysiology, which also apply to other neuroscientific techniques.

#2A Electrophysiology is well-suited for an open development model

Recording electrical signals from the brain is a straightforward proposition. In the simplest case, it only requires two conductors to measure a potential difference, a means of amplifying that difference, and a method to store changes in the amplified signal over time. The  necessary technologies have been around for over a century, with signals recorded on spools of paper{CITATION}. Today, mass-produced circuits costing a few dollars can be used to amplify neural signals and store them digitally{CITATION}.

While the technology for recording of neural signals has remained almost unchanged, the electrodes used to record the signals form the brain, and the data processing are evolving rapidly. In recent years, there has been a push to record from more channels simultaneously in order to understand the brain at the network level{CITATION - brain project etc}. Further, the emergence of increasingly specific ways of manipulating neural activity{CITATION} and a push towards more complex behavioural experiments requires equipment in which experiments are changed based on behavioural and neural data in real-time instead of being pre-determined.

For the most part, advances in recording technology occur within individual labs. Companies then take these advances, optimize them for general usability, and distribute them to the wider community. The resulting systems are typically monolithic, in that all the parts are designed by the same company, and closed, in that the hardware designs and source code are not publicly available. Some of the major vendors of commercial data acquisition systems are Neuralynx, Plexon, Blackrock, Tucker-Davis Technologies, Ripple, and Axona [any others to add???]. By giving researchers access to high-quality, professionally tested tools, as well as reliable support services, these companies have been essential to the spread of multichannel electrophysiology systems over the past two decades. However, it is no longer clear that these services should be provided exclusively by commercial entities.

We see three reasons why the current model of tool development and distribution would benefit from an active open-source community:

1. Electrode technology is advancing rapidly. Experimenters using twisted-wire tetrodes are packing more wires into a smaller area{CITATIONS}; silicon probes are becoming thinner and denser{CITATIONS}; and active probes are under development{CITATIONS}. Researchers need the flexibility to choose the option that is best-suited to their particular application, and need to be able to mix-and match electrode types, but vendor lock-in can prevent this. Companies have trouble keeping up with the latest advances, and when they do, they often implement proprietary standards that Balkanize the field. One recent example of this is the development of amplifier chips from Intan technologies, which can amplify and digitize 32 channels of neural data in an 8 x 8 mm package. When integrated into a "headstage" (the temporary interface that connects implanted electrodes to a data acquisition system), systems based on Intan chips offer considerable advantages over their analog counterparts. For this reason, nearly every major vendor now sells headstages that use Intan chips for digitization, but none of them are interchangeable. Users are stuck with whatever connectors the vendors have chosen to provide, and cannot customize them without the help of the manufacturer.

2. On the software side, the requirements for analysis and visualization vary greatly from researcher to researcher, and even from experiment to experiment. Specialized algorithms are needed to handle electrophysiological data once it reaches the computer, especially when closed-loop feedback is involved. It's often difficult for researchers to predict which algorithms will be necessary before the experiments have been run. An example of this is online spike-sorting, which allows researchers to probe the response properties of single units near their recording electrode. Certain commercial software may or may not implement this, and they may all use different algorithms. This makes it difficult or impossible to directly compare data collected across different platforms.

3. There is already a huge amount of development going on within each lab. Electrophysiologists tend to be technically savvy and favor a "do it yourself" approach to their work. Some of this is cultural, but much of it is out of necessity. The limiting nature of the commercially available systems has forced many electrophysiologists to take matters into their own hands, and develop customized hardware and software for their experiments. Unfortunately, very little of this development is shared, leading to a huge amount of redundant effort within and across laboratories.

These reasons, which are not unique to extracellular electrophysiology, make it likely that a shift toward a more open development model will occur in the near future.

#2B Open interfaces: a middle-of-the-road solution

We propose that an open approach to hardware and software development that centers around standardized, open interfaces and modular architecture could greatly improve the productivity of the scientific community.

This model does not conflict with the business model of commercial systems vendors. There is no reason why labs should pay any less the huge amount of technical expertise built by makers of commercial systems than they do now. However, in a model where system components are modular, well documented, and interchangeable, companies could diversify and concentrate their resources on the development of novel equipment, in collaboration with scientists that either require new tools, or already have prototypes that aren't ready for distribution. Additionally, standardized equipment should give rise to an even bigger market for professional support for existing systems, similar to how companies sell support for Linux based systems rather than selling the software itself.

There is also no fundamental reason why all components of scientific tools would need to be completely open source either. All current scientific tools make heavy use of 'black boxes' in the form of well documented integrated circuits that perform specified functions ranging from amplifiers and filters up to FPGAs and processors. As long as the behavior of a component is well defined, and its interfaces are documented and adhere to standards wherever possible, closed-source components are not necessarily an obstacle to the functionality of the whole system. The key requirement is that tools need to allow researchers to choose their path, and have options at each point.

The same principle applies to the software used to record data. 
While there have been efforts to standardize file formats recently{CITE}, the actual software currently is monolithic and tied to specific hardware.
This is becoming problematic for the same reasons that we need modular open-source hardware for: Higher channel counts require new data-processing during, not after the recording, and closed loop experiments.


On the analysis end, we have the opposite situation; most people write their own analysis code, and none of it is vetted by companies; it's all open, and optional to share; would be nice to move toward a middle ground here as well. There have been notable efforts in this direction {CITE} but as a whole the field is not doing nearly enough.

#2C Examples of open-source approaches

---

#2D Pros and cons of the open-source model

---

#3 Conclusion

---



